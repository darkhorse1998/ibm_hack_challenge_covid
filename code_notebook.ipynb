{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": "# The code was removed by Watson Studio for sharing."
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": "try:\n    import tweepy\nexcept Exception:\n    ! pip install tweepy\n    import tweepy"
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": "consumer_key = \"fUvMF6ZpAnC9PEW9rjfDaOVTK\"\nconsumer_secret = \"ZrqzWKT8bkmcwOHCrPozzbafnvdwOXj5WicMkXVA6Ctg9HSMLy\"\naccess_key = \"1085139034681344000-2j9Jom0bp2WDn6yZ9XYMyv1k4oa8jT\"\naccess_secret = \"QjbPNKGZQOaPKry2mh65c0vog6S2KRWlvYLspxi2FnPmX\""
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": "OAUTH_KEYS = {'consumer_key':consumer_key, 'consumer_secret':consumer_secret,\n 'access_token_key':access_key, 'access_token_secret':access_secret}\nauth = tweepy.OAuthHandler(OAUTH_KEYS['consumer_key'], OAUTH_KEYS['consumer_secret'])\napi = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "# IMPORTING LIBRARIES"
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": "import pandas as pd\nimport numpy as np\nimport time\nimport json"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "# EXTRACTING TWEETS"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Covid-19"
        },
        {
            "cell_type": "code",
            "execution_count": 28,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": "Rate limit reached. Sleeping for: 64\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "corona\n"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": "Rate limit reached. Sleeping for: 754\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "corona virus\n"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": "Rate limit reached. Sleeping for: 751\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "covid19\ncovid-19\n"
                },
                {
                    "data": {
                        "text/plain": "{'file_name': 'corona_retrieved_data1.json',\n 'message': 'File saved to project storage.',\n 'bucket_name': 'sentimentanalysisoncovid19tweets-donotdelete-pr-ltfnobw22qpnyw',\n 'asset_id': '3915bf25-8e33-45c4-82f1-35a4fbc8a5c4'}"
                    },
                    "execution_count": 28,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "# # it is important to know that we won't be extracting media data from Twitter and hence we will be using 'filters' to filter out media\n# # the tweets' topics can be regarded as a query and given in form of a list\n\n# filters = '-filter:media'\n# query_words = ['corona','corona virus','covid19','covid-19']\n# for i in range(len(query_words)):\n#     search = tweepy.Cursor(api.search, q = query_words[i]+filters, \n#                        tweet_mode = 'extended',\n#                        include_rts = True,\n#                        lang = \"en\").items(5000) \n    \n#     all_data = []\n#     for tweet in search:\n#         status = tweet\n#         all_data.append(tweet._json)     \n\n    \n#     project.save_data(data = json.dumps(all_data),file_name = query_words[i]+'_data.json',overwrite = True)    \n#     print(query_words[i])\n#     time.sleep(1)\n\n# # compiling all tweets together\n# project.save_data(data = json.dumps(all_data),file_name = 'corona_retrieved_data1.json',overwrite = True)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Healthcare and Religion"
        },
        {
            "cell_type": "code",
            "execution_count": 33,
            "metadata": {},
            "outputs": [],
            "source": "# # it is important to know that we won't be extracting media data from Twitter and hence we will be using 'filters' to filter out media\n# # the tweets' topics can be regarded as a query and given in form of a list\n\n# filters = '-filter:media'\n# query_words = ['WHO','World Health Organisation','Religion','Hindu Muslim','USAID']\n# for i in range(len(query_words)):\n#     search = tweepy.Cursor(api.search, q = query_words[i]+filters, \n#                        tweet_mode = 'extended',\n#                        include_rts = True,\n#                        lang = \"en\").items(2000) \n    \n#     all_data = []\n#     for tweet in search:\n#         status = tweet\n#         all_data.append(tweet._json)     \n\n    \n#     project.save_data(data = json.dumps(all_data),file_name = query_words[i]+'_data2.json',overwrite = True)    \n#     print(query_words[i])\n#     time.sleep(1)\n\n# # compiling all tweets together\n# project.save_data(data = json.dumps(all_data),file_name = 'corona_retrieved_data2.json',overwrite = True)"
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "covid-19 vaccine\nvaccine\ncovid-19 cure\n"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": "Rate limit reached. Sleeping for: 747\n"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "cure\nfake news\n"
                },
                {
                    "data": {
                        "text/plain": "{'file_name': 'corona_retrieved_data3.json',\n 'message': 'File saved to project storage.',\n 'bucket_name': 'sentimentanalysisoncovid19tweets-donotdelete-pr-ltfnobw22qpnyw',\n 'asset_id': 'a62963b5-238b-453d-a353-144c0aa0c10c'}"
                    },
                    "execution_count": 21,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "# # it is important to know that we won't be extracting media data from Twitter and hence we will be using 'filters' to filter out media\n# # the tweets' topics can be regarded as a query and given in form of a list\n\n# filters = '-filter:media'\n# query_words = ['covid-19 vaccine','vaccine','covid-19 cure','cure', 'fake news']\n# for i in range(len(query_words)):\n#     search = tweepy.Cursor(api.search, q = query_words[i]+filters, \n#                        tweet_mode = 'extended',\n#                        include_rts = True,\n#                        lang = \"en\").items(2000) \n    \n#     all_data = []\n#     for tweet in search:\n#         status = tweet\n#         all_data.append(tweet._json)     \n\n    \n#     project.save_data(data = json.dumps(all_data),file_name = query_words[i]+'_data3.json',overwrite = True)    \n#     print(query_words[i])\n#     time.sleep(1)\n\n# # compiling all tweets together\n# project.save_data(data = json.dumps(all_data),file_name = 'corona_retrieved_data3.json',overwrite = True)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": ""
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": ""
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.6",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.6.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}